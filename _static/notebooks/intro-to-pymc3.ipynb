{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPython.matplotlib.backend = \"retina\"\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"figure.dpi\"] = 300\n",
    "rcParams[\"savefig.dpi\"] = 300\n",
    "\n",
    "from celerite import plot_setup\n",
    "plot_setup.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python: Modeling\n",
    "\n",
    "This tutorial will demonstrate how you can use the modeling protocol in celerite to fit for the mean parameters simultaneously with the kernel parameters.\n",
    "\n",
    "In this example, we’re going to simulate a common data analysis situation where our dataset exhibits unknown correlations in the noise. When taking data, it is often possible to estimate the independent measurement uncertainty on a single point (due to, for example, Poisson counting statistics) but there are often residual systematics that correlate data points. The effect of this correlated noise can often be hard to estimate but ignoring it can introduce substantial biases into your inferences. In the following sections, we will consider a synthetic dataset with correlated noise and a simple non-linear model. We will fit these data by modeling the covariance structure in the data using a Gaussian process.\n",
    "\n",
    "\n",
    "## A Simple Mean Model\n",
    "\n",
    "The model that we’ll fit in this demo is a single Gaussian feature with three parameters: amplitude $\\alpha$, location $\\ell$, and width $\\sigma^2$. I’ve chosen this model because is is the simplest non-linear model that I could think of, and it is qualitatively similar to a few problems in astronomy (fitting spectral features, measuring transit times, etc.).\n",
    "\n",
    "\n",
    "## Simulated Dataset\n",
    "\n",
    "Let's start by simulating a dataset of 50 points with known correlated noise. In fact, this example is somewhat artificial since the data were drawn from a Gaussian process but in everything that follows, we’ll use a different kernel function for our inferences in an attempt to make the situation slightly more realistic. A known white variance was also added to each data point.\n",
    "\n",
    "Using the parameters\n",
    "\n",
    "$$\\alpha = −1 \\quad, \\quad\\quad \\ell = 0.1 \\quad, \\quad\\quad \\sigma^2 = 0.4 \\quad.$$\n",
    "\n",
    "the resulting dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from celerite.modeling import Model\n",
    "\n",
    "# Define the model\n",
    "class MeanModel(Model):\n",
    "    parameter_names = (\"alpha\", \"ell\", \"log_sigma2\")\n",
    "    \n",
    "    def get_value(self, t):\n",
    "        return self.alpha * np.exp(-0.5*(t-self.ell)**2 * np.exp(-self.log_sigma2))\n",
    "    \n",
    "    # This method is optional but it can be used to compute the gradient of the\n",
    "    # cost function below.\n",
    "    def compute_gradient(self, t):\n",
    "        e = 0.5*(t-self.ell)**2 * np.exp(-self.log_sigma2)\n",
    "        dalpha = np.exp(-e)\n",
    "        dell = self.alpha * dalpha * (t-self.ell) * np.exp(-self.log_sigma2)\n",
    "        dlog_s2 = self.alpha * dalpha * e\n",
    "        return np.array([dalpha, dell, dlog_s2])\n",
    "\n",
    "mean_model = MeanModel(alpha=-1.0, ell=0.1, log_sigma2=np.log(0.4))\n",
    "true_params = mean_model.get_parameter_vector()\n",
    "\n",
    "# Simuate the data\n",
    "np.random.seed(42)\n",
    "x = np.sort(np.random.uniform(-5, 5, 50))\n",
    "yerr = np.random.uniform(0.05, 0.1, len(x))\n",
    "K = 0.1*np.exp(-0.5*(x[:, None] - x[None, :])**2/10.5)\n",
    "K[np.diag_indices(len(x))] += yerr**2\n",
    "y = np.random.multivariate_normal(mean_model.get_value(x), K)\n",
    "\n",
    "# Plot the data\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.xlabel(r\"$t$\")\n",
    "plt.xlim(-5, 5)\n",
    "plt.gca().yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "plt.title(\"simulated data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the Noise\n",
    "\n",
    "**Note:** A full discussion of the theory of Gaussian processes is beyond the scope of this demo—you should probably check out [Rasmussen & Williams (2006)](http://www.gaussianprocess.org/gpml/)—but I'll try to give a quick qualitative motivation for our model.\n",
    "\n",
    "In this section, instead of assuming that the noise is white, we'll include covariances between data points.\n",
    "Under the assumption of Gaussianity, the likelihood function is\n",
    "\n",
    "$$\n",
    "    \\ln p(\\{y_n\\}\\,|\\,\\{t_n\\},\\,\\{\\sigma_n^2\\},\\,\\theta) =\n",
    "        -\\frac{1}{2}\\,\\boldsymbol{r}^\\mathrm{T}\\,K^{-1}\\,\\boldsymbol{r}\n",
    "        -\\frac{1}{2}\\,\\ln\\det K - \\frac{N}{2}\\,\\ln 2\\pi\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{r} = \\left ( \\begin{array}{c}\n",
    "        y_1 - f_\\theta(t_1) \\\\\n",
    "        y_2 - f_\\theta(t_2) \\\\\n",
    "        \\vdots \\\\\n",
    "        y_N - f_\\theta(t_N) \\\\\n",
    "    \\end{array}\\right)\n",
    "$$\n",
    "\n",
    "is the residual vector and $K$ is the $N \\times N$ data covariance matrix (where :math:`N` is the\n",
    "number of data points) with elements given by\n",
    "\n",
    "$$\n",
    "    K_{nm} = \\sigma_n^2\\,\\delta_{nm} + k(t_n,\\,t_m)\n",
    "$$\n",
    "\n",
    "where $\\delta_{ij}$ is the [Kronecker delta](http://en.wikipedia.org/wiki/Kronecker_delta) and $k(\\cdot,\\,\\cdot)$\n",
    "is a covariance function that we get to choose.\n",
    "We'll use a simple celerite `RealTerm`\n",
    "\n",
    "$$\n",
    "    k(\\tau_{nm}) = a \\, \\exp \\left (-c\\,\\tau_{nm} \\right )\n",
    "$$\n",
    "\n",
    "where $\\tau_{nm} = |t_n - t_m|$, and $a$ and $c$ are the\n",
    "parameters of the model.\n",
    "\n",
    "\n",
    "## The Fit\n",
    "\n",
    "Now we could go ahead and implement the likelihood function that we came up\n",
    "with in the previous section but celerite does that for us.\n",
    "To implement the model from the previous section, we can write\n",
    "the following likelihood function in Python and find it's maximum using scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "import celerite\n",
    "from celerite import terms\n",
    "\n",
    "# Set up the GP model\n",
    "kernel = terms.RealTerm(log_a=np.log(np.var(y)), log_c=-np.log(10.0))\n",
    "gp = celerite.GP(kernel, mean=mean_model, fit_mean=True)\n",
    "gp.compute(x, yerr)\n",
    "print(\"Initial log-likelihood: {0}\".format(gp.log_likelihood(y)))\n",
    "\n",
    "# Define a cost function\n",
    "def neg_log_like(params, y, gp):\n",
    "    gp.set_parameter_vector(params)\n",
    "    return -gp.log_likelihood(y)\n",
    "\n",
    "def grad_neg_log_like(params, y, gp):\n",
    "    gp.set_parameter_vector(params)\n",
    "    return -gp.grad_log_likelihood(y)[1]\n",
    "\n",
    "# Fit for the maximum likelihood parameters\n",
    "initial_params = gp.get_parameter_vector()\n",
    "bounds = gp.get_parameter_bounds()\n",
    "soln = minimize(neg_log_like, initial_params, jac=grad_neg_log_like,\n",
    "                method=\"L-BFGS-B\", bounds=bounds, args=(y, gp))\n",
    "gp.set_parameter_vector(soln.x)\n",
    "print(\"Final log-likelihood: {0}\".format(-soln.fun))\n",
    "\n",
    "# Make the maximum likelihood prediction\n",
    "t = np.linspace(-5, 5, 500)\n",
    "mu, var = gp.predict(y, t, return_var=True)\n",
    "std = np.sqrt(var)\n",
    "\n",
    "# Plot the data\n",
    "color = \"#ff7f0e\"\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.plot(t, mu, color=color)\n",
    "plt.fill_between(t, mu+std, mu-std, color=color, alpha=0.3, edgecolor=\"none\")\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.xlabel(r\"$t$\")\n",
    "plt.xlim(-5, 5)\n",
    "plt.gca().yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "plt.title(\"maximum likelihood prediction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit this model using MCMC (using [emcee](http://dfm.io/emcee)), we need to first choose priors—in this case we’ll just use a simple uniform prior on each parameter—and then combine these with our likelihood function to compute the log probability (up to a normalization constant). In code, this will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probability(params):\n",
    "    gp.set_parameter_vector(params)\n",
    "    lp = gp.log_prior()\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return gp.log_likelihood(y) + lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model implemented, we’ll initialize the walkers and run both a burn-in and production chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "initial = np.array(soln.x)\n",
    "ndim, nwalkers = len(initial), 32\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability)\n",
    "\n",
    "print(\"Running burn-in...\")\n",
    "p0 = initial + 1e-8 * np.random.randn(nwalkers, ndim)\n",
    "p0, lp, _ = sampler.run_mcmc(p0, 500)\n",
    "\n",
    "print(\"Running production...\")\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(p0, 2000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the chain, we can plot the predicted results. It is often useful to plot the results on top of the data as well. To do this, we can over plot 24 posterior samples on top of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data.\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "\n",
    "# Plot 24 posterior samples.\n",
    "samples = sampler.flatchain\n",
    "for s in samples[np.random.randint(len(samples), size=24)]:\n",
    "    gp.set_parameter_vector(s)\n",
    "    mu = gp.predict(y, t, return_cov=False)\n",
    "    plt.plot(t, mu, color=color, alpha=0.3)\n",
    "    \n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.xlabel(r\"$t$\")\n",
    "plt.xlim(-5, 5)\n",
    "plt.gca().yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "plt.title(\"posterior predictions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this figure, the data are shown as black points with error bars and the posterior samples are shown as translucent orange lines. These results seem pretty satisfying but, since we know the true model parameters that were used to simulate the data, we can assess the fit by comparing the inferences to the true values. To do this, we’ll plot all the projections of our posterior samples using [corner.py](https://github.com/dfm/corner.py) and over plot the true values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "names = gp.get_parameter_names()\n",
    "cols = mean_model.get_parameter_names()\n",
    "inds = np.array([names.index(\"mean:\"+k) for k in cols])\n",
    "corner.corner(sampler.flatchain[:, inds], truths=true_params,\n",
    "              labels=[r\"$\\alpha$\", r\"$\\ell$\", r\"$\\log\\sigma^2$\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from this figure that the constraints obtained for the mean parameters when modeling the noise are reasonable even though our noise model was \"wrong\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
